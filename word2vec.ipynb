{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martikvm/anaconda3/envs/py35/lib/python3.5/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient\n",
    "import urllib.parse\n",
    "import pyspark.sql.functions as sqlf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import MinHashLSH, NGram, Tokenizer, CountVectorizer, BucketedRandomProjectionLSH\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, ArrayType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from DisjointSet import DisjointSet\n",
    "import sys\n",
    "sys.setrecursionlimit(3000000)\n",
    "\n",
    "normalized_file = 'normalizedTweetsWithoutLinks_2019_test.txt'\n",
    "results_file = 'results/TweetsAlike_2019_word2vec.txt'\n",
    "beautify_results_file = 'results/niceResults_2019_word2vec.txt'\n",
    "\n",
    "credPath = 'credentialsMongo.txt'\n",
    "credFile = open(credPath, 'r')\n",
    "loginMA = urllib.parse.quote_plus(credFile.readline().strip())\n",
    "passwordMA = urllib.parse.quote_plus(credFile.readline()).strip()\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"twitter\") \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.3.0') \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb+srv://\" +\n",
    "            loginMA + \":\" +\n",
    "            passwordMA +\n",
    "            \"@doublesearchintwitter-m3qge.mongodb.net/twitter_march2019.tweets\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb+srv://\" +\n",
    "            loginMA + \":\" +\n",
    "            passwordMA +\n",
    "            \"@doublesearchintwitter-m3qge.mongodb.net/\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/Users/martikvm/anaconda3/envs/py35/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.text(normalized_file).withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('/Users/martikvm/PycharmProjects/DoubleSearch/twitter/model.bin', binary=True)\n",
    "#model = KeyedVectors.load_word2vec_format('/Users/martikvm/PycharmProjects/DoubleSearch/twitter/model1.model', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+\n",
      "|               value| id|               words|\n",
      "+--------------------+---+--------------------+\n",
      "|\"- Ох, Анечка, ес...|  0|[\"-, ох,, анечка,...|\n",
      "|твой путь (Владим...|  1|[твой, путь, (вла...|\n",
      "|[user] Я уж руки ...|  2|[[user], я, уж, р...|\n",
      "|[user] [user] [us...|  3|[[user], [user], ...|\n",
      "|[user] У меня нет...|  4|[[user], у, меня,...|\n",
      "|  [user] Так что, да|  5|[[user], так, что...|\n",
      "|[user] [user] [us...|  6|[[user], [user], ...|\n",
      "|Ещё одна Юля, тол...|  7|[ещё, одна, юля,,...|\n",
      "|[user] Я перемаза...|  8|[[user], я, перем...|\n",
      "|[user] Это что ща...|  9|[[user], это, что...|\n",
      "|чувак хочет кожан...| 10|[чувак, хочет, ко...|\n",
      "|                ОКЕЙ| 11|              [окей]|\n",
      "|сука знаете что к...| 12|[сука, знаете, чт...|\n",
      "|Украинцев ждет но...| 13|[украинцев, ждет,...|\n",
      "|    #Здоровье [link]| 14| [#здоровье, [link]]|\n",
      "|побейте меня если...| 15|[побейте, меня, е...|\n",
      "|надеюсь все прави...| 16|[надеюсь, все, пр...|\n",
      "|За 2 недели до сд...| 17|[за, 2, недели, д...|\n",
      "|Ой знакомое что-т...| 18|[ой, знакомое, чт...|\n",
      "|2 мин и выкл. Ну ...| 19|[2, мин, и, выкл....|\n",
      "+--------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"words\")\n",
    "pipeline = Pipeline(stages=[tokenizer])\n",
    "model1 = pipeline.fit(df1)\n",
    "df2 = model1.transform(df1)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|               value|               words|            features|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|\"- Ох, Анечка, ес...|[\"-, ох,, анечка,...|[-0.0100879698681...|\n",
      "|  1|твой путь (Владим...|[твой, путь, (вла...|[0.01080389427287...|\n",
      "|  2|[user] Я уж руки ...|[[user], я, уж, р...|[0.03005377762019...|\n",
      "|  3|[user] [user] [us...|[[user], [user], ...|[-0.0331375510742...|\n",
      "|  4|[user] У меня нет...|[[user], у, меня,...|[0.03763646715217...|\n",
      "|  5|  [user] Так что, да|[[user], так, что...|[-0.1999965831637...|\n",
      "|  6|[user] [user] [us...|[[user], [user], ...|[0.02100751442568...|\n",
      "|  7|Ещё одна Юля, тол...|[ещё, одна, юля,,...|[-0.0146749243140...|\n",
      "|  8|[user] Я перемаза...|[[user], я, перем...|[-8.0795213580131...|\n",
      "|  9|[user] Это что ща...|[[user], это, что...|[-0.0926305919885...|\n",
      "| 10|чувак хочет кожан...|[чувак, хочет, ко...|[-0.0597356206604...|\n",
      "| 11|                ОКЕЙ|              [окей]|[-0.2082820236682...|\n",
      "| 12|сука знаете что к...|[сука, знаете, чт...|[-0.0333644362166...|\n",
      "| 13|Украинцев ждет но...|[украинцев, ждет,...|[-0.0129442187872...|\n",
      "| 14|    #Здоровье [link]| [#здоровье, [link]]|                null|\n",
      "| 15|побейте меня если...|[побейте, меня, е...|[-0.0354547348635...|\n",
      "| 16|надеюсь все прави...|[надеюсь, все, пр...|[-0.0655572959221...|\n",
      "| 17|За 2 недели до сд...|[за, 2, недели, д...|[-0.0133566461736...|\n",
      "| 18|Ой знакомое что-т...|[ой, знакомое, чт...|[-0.0252047735266...|\n",
      "| 19|2 мин и выкл. Ну ...|[2, мин, и, выкл....|[0.00617395229637...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getVectors(v):\n",
    "    sum_vectors = np.zeros(300)\n",
    "    count = len(v)\n",
    "    for i in v:\n",
    "        try: \n",
    "            sum_vectors += model.get_vector(i + \"_NOUN\")\n",
    "        except:\n",
    "            try:\n",
    "                sum_vectors += model.get_vector(i + \"_VERB\")\n",
    "            except:\n",
    "                try:\n",
    "                    sum_vectors += model.get_vector(i + \"_PROPN\")\n",
    "                except:\n",
    "                    try:\n",
    "                        sum_vectors += model.get_vector(i + \"_INTJ\")\n",
    "                    except:\n",
    "                        try:\n",
    "                            sum_vectors += model.get_vector(i + \"_ADV\")\n",
    "                        except:\n",
    "                            try:\n",
    "                                sum_vectors += model.get_vector(i + \"_ADJ\")\n",
    "                            except:\n",
    "                                try:\n",
    "                                    sum_vectors += model.get_vector(i + \"_ADP\")\n",
    "                                except:\n",
    "                                    try:\n",
    "                                        sum_vectors += model.get_vector(i + \"_AUX\")\n",
    "                                    except:\n",
    "                                        try:\n",
    "                                            sum_vectors += model.get_vector(i + \"_CCONJ\")\n",
    "                                        except:\n",
    "                                            try:\n",
    "                                                sum_vectors += model.get_vector(i + \"_DET\")\n",
    "                                            except:\n",
    "                                                try:\n",
    "                                                    sum_vectors += model.get_vector(i + \"_NUM\")\n",
    "                                                except:\n",
    "                                                    try:\n",
    "                                                        sum_vectors += model.get_vector(i + \"_PART\")\n",
    "                                                    except:\n",
    "                                                        try:\n",
    "                                                            sum_vectors += model.get_vector(i + \"_PRON\")\n",
    "                                                        except:\n",
    "                                                            try:\n",
    "                                                                sum_vectors += model.get_vector(i + \"_SCONJ\")\n",
    "                                                            except:\n",
    "                                                                try:\n",
    "                                                                    sum_vectors += model.get_vector(i + \"_PUNCT\")\n",
    "                                                                except:\n",
    "                                                                    try:\n",
    "                                                                        sum_vectors += model.get_vector(i + \"_SYM\")\n",
    "                                                                    except:\n",
    "                                                                        try:\n",
    "                                                                            sum_vectors += model.get_vector(i + \"_X\")\n",
    "                                                                        except:\n",
    "                                                                            pass\n",
    "    vector = Vectors.dense(sum_vectors / count)\n",
    "    if vector != Vectors.dense(model.get_vector('конь_NOUN') - model.get_vector('конь_NOUN')):\n",
    "        return vector\n",
    "    return\n",
    "\n",
    "       \n",
    "getVectors_udf = udf(getVectors, VectorUDT())\n",
    "df3 = df2.select(\"id\", \"value\", \"words\", getVectors_udf(\"words\").alias(\"features\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204010"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_not_Null = df3.na.drop()\n",
    "df3.count()\n",
    "df3_not_Null.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|               value|               words|            features|              hashes|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|\"- Ох, Анечка, ес...|[\"-, ох,, анечка,...|[-0.0100879698681...|[[1139.0], [-112....|\n",
      "|  1|твой путь (Владим...|[твой, путь, (вла...|[0.01080389427287...|[[-30.0], [158.0]...|\n",
      "|  2|[user] Я уж руки ...|[[user], я, уж, р...|[0.03005377762019...|[[72.0], [-1130.0...|\n",
      "|  3|[user] [user] [us...|[[user], [user], ...|[-0.0331375510742...|[[72.0], [-157.0]...|\n",
      "|  4|[user] У меня нет...|[[user], у, меня,...|[0.03763646715217...|[[-245.0], [-583....|\n",
      "|  5|  [user] Так что, да|[[user], так, что...|[-0.1999965831637...|[[-114.0], [-201....|\n",
      "|  6|[user] [user] [us...|[[user], [user], ...|[0.02100751442568...|[[78.0], [-605.0]...|\n",
      "|  7|Ещё одна Юля, тол...|[ещё, одна, юля,,...|[-0.0146749243140...|[[963.0], [-1300....|\n",
      "|  8|[user] Я перемаза...|[[user], я, перем...|[-8.0795213580131...|[[337.0], [-53.0]...|\n",
      "|  9|[user] Это что ща...|[[user], это, что...|[-0.0926305919885...|[[-729.0], [-587....|\n",
      "| 10|чувак хочет кожан...|[чувак, хочет, ко...|[-0.0597356206604...|[[-29.0], [-386.0...|\n",
      "| 11|                ОКЕЙ|              [окей]|[-0.2082820236682...|[[1005.0], [1207....|\n",
      "| 12|сука знаете что к...|[сука, знаете, чт...|[-0.0333644362166...|[[-189.0], [241.0...|\n",
      "| 13|Украинцев ждет но...|[украинцев, ждет,...|[-0.0129442187872...|[[342.0], [-300.0...|\n",
      "| 15|побейте меня если...|[побейте, меня, е...|[-0.0354547348635...|[[194.0], [-760.0...|\n",
      "| 16|надеюсь все прави...|[надеюсь, все, пр...|[-0.0655572959221...|[[-124.0], [-1303...|\n",
      "| 17|За 2 недели до сд...|[за, 2, недели, д...|[-0.0133566461736...|[[582.0], [458.0]...|\n",
      "| 18|Ой знакомое что-т...|[ой, знакомое, чт...|[-0.0252047735266...|[[760.0], [-57.0]...|\n",
      "| 19|2 мин и выкл. Ну ...|[2, мин, и, выкл....|[0.00617395229637...|[[-110.0], [-352....|\n",
      "| 20|есть у меня нехор...|[есть, у, меня, н...|[-0.0128537989221...|[[-11.0], [-70.0]...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "mh = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=0.0001,\n",
    "                                  numHashTables=128)\n",
    "model2 = mh.fit(df3_not_Null)\n",
    "transformed_df2 = model2.transform(df3_not_Null)\n",
    "transformed_df2.cache()\n",
    "transformed_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colour_tweets(rows_with_ids, edges):\n",
    "    cnt = 1\n",
    "    for row in rows_with_ids:\n",
    "        # print(row)\n",
    "        cnt += 1\n",
    "        \n",
    "        ids = row[\"ids\"]\n",
    "        for x in ids[1:]:\n",
    "            edges.union(ids[0], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+\n",
      "|               value| id|          hashes_sum|\n",
      "+--------------------+---+--------------------+\n",
      "|\"- Ох, Анечка, ес...|  0|[23751611, -14965...|\n",
      "|твой путь (Владим...|  1|[-1899228058, 168...|\n",
      "|[user] Я уж руки ...|  2|[-1582300116, 217...|\n",
      "|[user] [user] [us...|  3|[-1981635403, -10...|\n",
      "|[user] У меня нет...|  4|[-872658983, -158...|\n",
      "|  [user] Так что, да|  5|[-2035100643, 108...|\n",
      "|[user] [user] [us...|  6|[1346172505, -170...|\n",
      "|Ещё одна Юля, тол...|  7|[-1801389114, -16...|\n",
      "|[user] Я перемаза...|  8|[-97307669, -2117...|\n",
      "|[user] Это что ща...|  9|[-345547899, 4627...|\n",
      "|чувак хочет кожан...| 10|[-362199858, -971...|\n",
      "|                ОКЕЙ| 11|[1139921483, -668...|\n",
      "|сука знаете что к...| 12|[127745542, 16120...|\n",
      "|Украинцев ждет но...| 13|[320303415, -1615...|\n",
      "|побейте меня если...| 15|[-1889643217, -36...|\n",
      "|надеюсь все прави...| 16|[-669461349, -783...|\n",
      "|За 2 недели до сд...| 17|[2089111251, 2905...|\n",
      "|Ой знакомое что-т...| 18|[235879121, -1348...|\n",
      "|2 мин и выкл. Ну ...| 19|[-658715301, -146...|\n",
      "|есть у меня нехор...| 20|[-911192017, -112...|\n",
      "+--------------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def getHashColumnsAll(df0):\n",
    "    result = []\n",
    "    a = 1000\n",
    "    for y in range(len(df0) // 4):\n",
    "    #for y in range(1, 2):   \n",
    "        sum_of_hashes =\\\n",
    "            (int(df0[4 * y][0]) + a) * math.pow((2 * a + 1), 3) +\\\n",
    "            (int(df0[4 * y + 1][0]) + a) * math.pow((2 * a + 1), 2) +\\\n",
    "            (int(df0[4 * y + 2][0]) + a) * (2 * a + 1) +\\\n",
    "            int(df0[4 * y + 3][0] + a)\n",
    "        result.append(int(sum_of_hashes))\n",
    "    return result\n",
    "\n",
    "gethashsumsall_udf = udf(getHashColumnsAll, ArrayType(IntegerType()))\n",
    "\n",
    "df4_all_sums = transformed_df2.select(\n",
    "    \"value\",\n",
    "    \"id\",\n",
    "    gethashsumsall_udf(\"hashes\").alias(\"hashes_sum\")\n",
    ")\n",
    "\n",
    "df4_all_sums.cache()\n",
    "df4_all_sums.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 1\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 2\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 3\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 4\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 5\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 6\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 7\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 8\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 9\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 10\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 11\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 12\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 13\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 14\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 15\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 16\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 17\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 18\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 19\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 20\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 21\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 22\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 23\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 24\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 25\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 26\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 27\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 28\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 29\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 30\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n",
      "k = 31\n",
      "df4 cached\n",
      "We coloured some tweets! Way to go!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_tweet_id = df1.agg({\"id\": \"max\"}).collect()[0][0]\n",
    "edges = DisjointSet(max_tweet_id + 1)\n",
    "\n",
    "with open(results_file, 'w') as outf:     \n",
    "    for k in range(128 // 4):\n",
    "    #for k in range(0, 1):\n",
    "        print(\"k = \" + str(k))\n",
    "        \n",
    "        df4_group = df4_all_sums.groupBy(df4_all_sums.hashes_sum[k])\\\n",
    "            .agg(\n",
    "                sqlf.count('id').alias(\"num_tweets\"),\n",
    "                sqlf.collect_list(\"value\").alias(\"tweets_texts\"),\n",
    "                sqlf.collect_list(\"id\").alias(\"ids\")\n",
    "            )\\\n",
    "            .filter(col(\"num_tweets\") > 1)\n",
    "        \n",
    "        df4_group.cache()\n",
    "        print(\"df4 cached\")\n",
    "                        \n",
    "        colour_tweets(df4_group.select(\"ids\").collect(), edges)\n",
    "        print(\"We coloured some tweets! Way to go!\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"edges_word2vec.pickle\", 'wb') as f:\n",
    "    pickle.dump(edges, f)\n",
    "with open(\"edges_word2vec.vertices.pickle\", 'wb') as f:\n",
    "    pickle.dump(edges.vertices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|color|              tweets|\n",
      "+-----+--------------------+\n",
      "|  148|[Полицейский с Ру...|\n",
      "|  463|[Не спрашивайте ф...|\n",
      "|  471|[также исследован...|\n",
      "|  833|[обойду, если теб...|\n",
      "| 1088|[[user] я пришла ...|\n",
      "| 1238|[[user] В 8-9 я з...|\n",
      "| 1342|[ну да, либо молч...|\n",
      "| 1580|[[user] На неё бы...|\n",
      "| 1591|[учиться лишь бы ...|\n",
      "| 1645|[Явно нужно сдела...|\n",
      "| 1829|[хочу зелёные вол...|\n",
      "| 1959|[[user] И что? Ко...|\n",
      "| 2122|[[user] умоляю я ...|\n",
      "| 2142|[[user] ППХАХАХЧХ...|\n",
      "| 2366|[какой твой любим...|\n",
      "| 2659|[А лучше просто К...|\n",
      "| 2866|[Завтра бы домой ...|\n",
      "| 3175|[[user] Взвешиваю...|\n",
      "| 3749|        [Цена129грн]|\n",
      "| 3918|[[user] [user] Вы...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_color(ind):\n",
    "    return edges.find(ind) \n",
    "\n",
    "add_color_udf = udf(add_color, IntegerType())\n",
    "\n",
    "df11 = df1\\\n",
    "    .select(\n",
    "        df1.value.alias(\"tweet\"),\n",
    "        add_color_udf(df1.id).alias(\"color\")\n",
    "    )\\\n",
    "    .groupBy(\"color\")\\\n",
    "    .agg(sqlf.collect_list(\"tweet\").alias(\"tweets\"))\n",
    "df11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/similar tweets with word2vec.txt\", \"w\") as outf:\n",
    "    for row in df11.collect():\n",
    "        tweetsCount = len(row.tweets)\n",
    "        if tweetsCount <= 1:\n",
    "            continue\n",
    "        outf.write(\"============ color: \" + str(row.color) + \", tweets: \" + str(tweetsCount) + \"===========\\n\")\n",
    "        for tweet in row.tweets:\n",
    "            outf.write(tweet)\n",
    "            outf.write(\"\\n\")\n",
    "        outf.write(\"\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115549390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "dictOfDoubles = {}\n",
    "with open(\"results/similar tweets with word2vec.txt\", \"r\") as doublesFile:\n",
    "    for line in doublesFile:\n",
    "        lineStripped = line.strip()\n",
    "        if lineStripped.startswith(\"============ color: \"):\n",
    "            endpoint1 = lineStripped.find(\", tweets:\")\n",
    "            #print(colour)\n",
    "            colour = int(lineStripped[20:endpoint1])\n",
    "            lineHalf = lineStripped[20:]\n",
    "            startpoint2 = endpoint1 + 9\n",
    "            endpoint2 = 20 + lineHalf.find(\"===========\")\n",
    "            tweets = int(lineStripped[startpoint2:endpoint2])\n",
    "            #print(tweets)\n",
    "            dictOfDoubles[colour] = tweets\n",
    "\n",
    "keys = list(dictOfDoubles.keys())\n",
    "vals = list(dictOfDoubles.values())\n",
    "plt.hist(vals, bins=list(range(100)))\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}